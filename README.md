# pro2ml 

###### 用matplot画图

[huatu.py](huatu.py) 简单折线图

![image-20250201234708725](README.assets/image-20250201234708725.png)



绘制基本图像 [basic_pic.py](basic_pic.py) 

一张图片同时绘制两个坐标图 [two_pic.py](two_pic.py) 

剩余部分见[教程 ](https://www.runoob.com/matplotlib/matplotlib-line.html)



###### numpy 

[np_1.py](np_1.py) 常用numpy数组的构建

[np_2_compute.py](np_2_compute.py)numpy数组的运算和统计方法



下图为广播机制,满足下列其中一个条件即可触发广播,扩充短的维度为长的

1数组的某一维度等长
2其中一个数组的某一维度为1。



![image-20250208005014542](README.assets/image-20250208005014542.png)

对于np.dot和np.matmul,都是矩阵乘法区别在于

- *np.dot*可以处理一维数组和标量的乘法，而*np.matmul*不能。
- 在处理二维数组时，*np.dot*和*np.matmul*的结果相同，都是矩阵乘积。
- *np.matmul*在处理高维数组时更加灵活，它可以处理多个数组的矩阵乘积，并且可以自动广播数组以匹配维度。

如果你需要计算标量或一维数组的点积，或者是两个二维数组的矩阵乘积，*np.dot*是合适的选择。如果你需要处理高维数组的矩阵乘积，并且希望利用自动广播的特性，那么*np.matmul*将是更好的选择



### ML

通过数据+算法实现功能,而不是仅靠基于规则的学习(if..else),让程序自己从数据中提取特征

##### 监督学习

给出特征值(feature)和目标值(labal),训练能完成分类或回归的模型

###### 分类:结果是已知的离散值

###### 回归:结果为连续值

##### 无监督学习

所给数据没有目标值,只有特征值

###### 聚类:为已有数据进行分类

###### 数据降维:清除噪声,压缩数据

##### 半监督学习

利用**少量**已有**标签**的数据训练模型
再利用该模型套用未标记数据
最后进行人工优化

###### 纯半监督学习:未标记样本不是待测数据

###### 直推学习(transductive learning):未标记样本就是待测数据

##### 强化学习RL

强化学习不像无监督学习那样完全没有学习目标，又不像监督学习那样有非常明确的目标（即
lbel),强化学习的目标一般是变化的、不明确的，甚至可能不存在绝对正确的标签
一般强化学习通过设置一个Agent,通过与环境的交互来获得奖励或惩罚,目标是长期利益最大化

##### 欠拟合

训练集上表现差:一般是模型过于简单
下述方法可以解决或者缓解欠拟合
 ①添加新特征
 ②复杂化模型
 ③减少正则化

##### 过拟合

 ①清洗数据
 ②增大训练数据量
 ③正则化
 ④dropout
 ⑤早停

##### 标准化

$$
X_i=\frac{x_i - mean}{\sigma_i}
$$

标准化后数据均值为0,标准差为1,

```python
from sklearn.preprocessing import StandardScaler
transformer =StandardScaler()
data=transformer.fit_transform(data)
```

可以通过上述代码实现特征归一化







#### sklearn

一个机器学习包,提供 **分类,回归,聚类,降维(特征工程),模型选择,调优** 六大模块

#### 1.1 K-近邻算法(KNN)概念 [k_neighbor.py](k_neighbor.py) 

K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法，总体来说KNN算法
是相对比较容易理解的算法

##### ·定义

样本类别定义为k个距离最近的样本中的众数的类别
KNN使用欧氏距离



##### 距离度量

###### 欧氏距离：

$$
d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
$$

###### 曼哈顿距离

![image-20250208104252227](README.assets/image-20250208104252227.png) 

黄色,蓝色,红色为曼哈顿距离,公式如下
$$
d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} \left| x_i - y_i \right|
$$

###### 切比雪夫距离



$$
d(\mathbf{x}, \mathbf{y}) = \max_{1 \leq i \leq n} \left| x_i - y_i \right|
$$

###### 闵可夫斯基距离

$$
d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{n} \left| x_i - y_i \right|^p \right)^{\frac{1}{p}}
$$

p=1时为曼哈顿距离

p=2时为欧氏距离

P→∞时为切比雪夫距离

上述距离存在一些问题,由于数据量纲不同,直接套用距离显然不太合适

###### 标准化欧氏距离公式

标准化欧氏距离在计算欧氏距离时考虑了各个特征的尺度差异，其公式为：

$$
d(\mathbf{x}, \mathbf{y}) = \sqrt{ \sum_{i=1}^{n} \left(\frac{x_i - y_i}{\sigma_i}\right)^2 }
$$

$$
\sigma_i 为第 i 个特征的标准差。
$$

这种距离度量在特征具有不同单位或量纲时非常有用，通过先对各个特征进行标准化，再计算欧氏距离，从而消除了不同尺度带来的影响。



###### 余弦距离

$$
\text{cosine similarity} = \frac{\sum_{i=1}^{n} X_i Y_i}{\sqrt{\sum_{i=1}^{n} X_i^2}\sqrt{\sum_{i=1}^{n} Y_i^2}}
$$



还有汉明距离,杰卡德距离,马氏距离...



##### 连续属性与离散属性

若属性值之间存在序关系，则可以将其转化为连续值，例如：身高属性“高”“中等“矮”，可转化为
{1,0.5,0}
			闵可夫斯基距离可以用于有序属性，
若属性值之间不存在序关系，则通常将其转化为向量的形式，例如：性别属性"男"“女”，可转化为
{0,1}

##### k值的选择

**近似误差**:训练集误差,太小可能过拟合
**估计误差** :测试集误差,越低越好

·K值过小：
。容易受到异常点的影响(过拟合
k值过大：
。受到样本均衡的问题,样本分布构建的模型有问题(模型太简单
取极端例子,k=n时任何预测均为样本的众数,失去了判断能力

一般而言,k值的选择通过交叉验证法找出最优k值

##### kd树

kd树在维度<20时效率较高,更高维度的数据可以使用ball tree

目的:优化k临近算法,提高knn的搜索效率,减少距离计算次数
原理:AB远,BC近,则AC远,从而降低运算复杂度

将时间复杂度从O(D*N^2)降低至O(DNlogN)

构造:通过对k维空间进行切分(超平面),用**中位数**递归生成子节点,产生一颗**平衡二叉树**
一般而言,选择分割的维度时会选择数据较为分散的维度(用**方差**来判断)
构建时**相邻两层的划分维度必定不相同**(正交划分)

搜索:从根节点向下遍历,找出最终归属的区域,与对应的节点求出距离即可(logN)

下面左图为辅助理解超平面,右图为树结构,⭐为要查找的点

**不跨域搜索:**

![](README.assets/image-20250208163847312.png)

不跨域是因为(5,4)到(4,7)的距离＞最小距离,因此无需查找另外一侧

**跨域搜索:**

![image-20250208163935916](README.assets/image-20250208163935916.png)

跨域搜索(2,3)是因为当前最小距离>(5,4)到(2,3)的距离

上图所说的画圆只是辅助理解,实际上还是求两点间距离

搜索过程:
①从根节点开始,比较**待查询节点和分裂节点的分裂维的值**(小于进左子树,大于进右子树
②**沿着搜索路径找到最近邻近似点**
③**回溯搜索路径**,如果存在可能比当前距离更近的点,则跳转到其他子节点空间
④重复上述过程直至**搜索路径**为空



##### 损失函数

用于衡量数据拟合程度
**线性拟合**的损失函数一般用最小二乘法

损失函数公式：(欧式距离的平方)
$$
J(w) = \sum_{i=1}^{m} (h(x_i) - y_i)^2
$$
正则化,标准化数据  [Standard.py](Standard.py) 



#####  [knn_2.py](knn_2.py) 使用knn对提供的数据集进行拟合和划分

在程序中存在两个函数fit_transform()和transform()

**`fit_transform()` 方法：**

- **功能**：首先对数据进行拟合（`fit`），计算所需的统计量（如均值、标准差等），然后立即对数据进行转换（`transform`）。

  在上述代码中，`fit_transform()` 首先计算训练数据 `X_train` 的均值和标准差，然后使用这些统计量对 `X_train` 进行标准化处理。

**`transform()` 方法：**

- **功能**：使用在 `fit` 阶段计算得到的统计量，对新的数据进行转换。
- **使用场景**：用于测试数据集或任何新的数据集，以**确保训练集和测试集使用相同的转换标准**。

因此程序中

```python
x_train = transfer.fit_transform(x_train)
# x_test = transfer.fit_transform(x_test)
x_test = transfer.transform(x_test)#因为都使用标准化方法,因此上下二式子等价(相同fit)
```

最后两行等价

##### 总结

###### 优点：

。简单有效
。重新**训练的代价低**
。**适合类域交叉样本**(不同类别的样本存在交叉或重叠的情况)
	·KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
。**适合大样本自动分类**
	·该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算
法比较容易产生误分。

###### ·缺点：

。惰性学习
·KNN算法是懒散学习方法(lazy learning,**基本上不学习**)，一些积极学习的算法要快很多
。类别**评分不是规格化**,没有分类概率
。输出**可解释性不强**
。对**不擅长不均衡的样本**

##### 交叉验证

数据分成k组,每次取1组作为验证集,其他作为测试集,求准确率;不同测试集验证k次后结果求平均即为交叉验证的准确率

##### 网格搜索

手动设置超参数过于复杂,因此通过对模型预设的超参数组合进行交叉验证,选出最优参数组合较为合理,下面是网格搜索代码



# [TODO]()

